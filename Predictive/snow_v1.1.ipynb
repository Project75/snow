{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d6fb32fd69316596e236eab5fb8cf77c848508c3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dataframe Shape(rows,cols): (62502, 2)\n"
     ]
    }
   ],
   "source": [
    "xlsx = pd.ExcelFile('user_incidents_dumps_60days.xlsx')\n",
    "#fields = ['Short description', 'Assignment group']\n",
    "\n",
    "data_sheets = []\n",
    "for sheet in xlsx.sheet_names:\n",
    "    data_sheets.append(pd.read_excel(xlsx,sheet, usecols=[3,6]))\n",
    "df = pd.concat(data_sheets)\n",
    "\n",
    "print(\"Input Dataframe Shape(rows,cols):\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace space in header with _\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Short_description</th>\n",
       "      <th>Assignment_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Network Account || Unable to login || Account ...</td>\n",
       "      <td>DH-Enterprise IT Service Cntr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ms4 -  phxasp01 - job needs to be killed / job...</td>\n",
       "      <td>DH-Enterprise IT Service Cntr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Printer || Unable to print the strips  through...</td>\n",
       "      <td>DH-NC-EUS Stockton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clairiva - unable to login / user id: ltimpog ...</td>\n",
       "      <td>DH-Enterprise IT Service Cntr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WOW || Unable to turn on || Black Screen || De...</td>\n",
       "      <td>DH-SC-EUS Bakersfield MSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Short_description  \\\n",
       "0  Network Account || Unable to login || Account ...   \n",
       "1  Ms4 -  phxasp01 - job needs to be killed / job...   \n",
       "2  Printer || Unable to print the strips  through...   \n",
       "3  Clairiva - unable to login / user id: ltimpog ...   \n",
       "4  WOW || Unable to turn on || Black Screen || De...   \n",
       "\n",
       "                Assignment_group  \n",
       "0  DH-Enterprise IT Service Cntr  \n",
       "1  DH-Enterprise IT Service Cntr  \n",
       "2             DH-NC-EUS Stockton  \n",
       "3  DH-Enterprise IT Service Cntr  \n",
       "4      DH-SC-EUS Bakersfield MSH  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47373, 2)\n"
     ]
    }
   ],
   "source": [
    "#drop duplicate rows\n",
    "df2 = df.drop_duplicates()\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe size after filtering(rows,cols): (39121, 2)\n"
     ]
    }
   ],
   "source": [
    "#get data with at leaset count(assignment group)>200\n",
    "df2=df2.groupby(\"Assignment_group\").filter(lambda x: len(x) > 200)\n",
    "print(\"dataframe size after filtering(rows,cols):\",df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DH-Enterprise IT Service Cntr        24901\n",
       "DHE-HR Tier 2 - Talent Management     1164\n",
       "DH-SW-EUS StJoseph                     923\n",
       "DH-Helpdesk RRE                        791\n",
       "DH-ClinApps NAS                        748\n",
       "DH-MPS Kyocera                         659\n",
       "DH-SW-EUS Chandler                     602\n",
       "DH-NC-EUS Redding                      513\n",
       "DH-SC-EUS StJohnRMC                    471\n",
       "DHE-SecAdmin                           461\n",
       "DH-SW-EUS Phoenix                      442\n",
       "DH-NC-EUS Stockton                     440\n",
       "DH-SC-EUS Bakersfield BMH              399\n",
       "DH-GB-EUS Dominican                    374\n",
       "DH-NC-EUS Sac MET                      365\n",
       "DH-SC-EUS NLA NrthrdgRoscoe            363\n",
       "DHE-RCM-AppOpSupport-MS4               360\n",
       "DH-ClinApps CPOE                       354\n",
       "DH-NC-EUS Sac MGH                      338\n",
       "DH-SW-EUS Gilbert                      328\n",
       "DH-SC-EUS SLA CalifornHospMC           293\n",
       "DH-SW-EUS StRoseSiena                  282\n",
       "DH-NC-Telcom Sacramento                277\n",
       "DH-ClinApps Pharmacy                   275\n",
       "DH-NC-EUS Sac MSJH                     269\n",
       "DH-NC-EUS Merced                       267\n",
       "DH-SC-EUS Marian                       257\n",
       "DH-SC-EUS SLA StMary                   234\n",
       "DH-SC-EUS-SBD-Community                232\n",
       "DH-SC-EUS Bakersfield MHB              232\n",
       "DH-ClinApps Lab                        232\n",
       "DH-SW-EUS DHMG                         221\n",
       "DH-GB-EUS StMaryMedCenter              216\n",
       "DH-NC-EUS Sac DDO                      215\n",
       "DH-GB-EUS Sequoia                      211\n",
       "DHE-Information Security Ops           209\n",
       "DH-ClinApps HIM                        203\n",
       "Name: Assignment_group, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print count of assignment groups\n",
    "df2.Assignment_group.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output classes:  37\n"
     ]
    }
   ],
   "source": [
    "#No. of unique assignment groups =  num of output classes\n",
    "num_classes = df2.Assignment_group.nunique()\n",
    "print(\"Number of output classes: \",num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data to pickle format if need to use later\n",
    "df1.to_pickle(\"snow_v1_43k_20out_0910.pkl\")\n",
    "#df = pd.read_pickle(\"snow_v1_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create input and output data sets\n",
    "X = df2.Short_description\n",
    "Y = df2.Assignment_group\n",
    "#print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465446"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word count\n",
    "sum([len(s.split()) for s in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400329\n"
     ]
    }
   ],
   "source": [
    "#remove stop words\n",
    "X1 = X.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "print(sum([len(s.split()) for s in X1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351291\n"
     ]
    }
   ],
   "source": [
    "#remove special chars\n",
    "pat = r'[^A-Za-z0-9 ]+'\n",
    "X2 = X1.str.replace(pat, ' ',regex=True)\n",
    "print(sum([len(s.split()) for s in X2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351291\n"
     ]
    }
   ],
   "source": [
    "#stemming words\n",
    "X3 = X2.apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "print(sum([len(s.split()) for s in X3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39121,) (39121,) (39121,) (39121,)\n"
     ]
    }
   ],
   "source": [
    "print(X2.shape,X3.shape,X1.shape,X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG:      Network Account || Unable to login || Account Locked || unlocked Account || Customer able to login\n",
      "STOPWORD:  Network Account || Unable login || Account Locked || unlocked Account || Customer able login\n",
      "SP. CHAR:  Network Account   Unable login   Account Locked   unlocked Account   Customer able login\n",
      "STEM:      network account unabl login account lock unlock account custom abl login\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIG:     \",X[0])\n",
    "print(\"STOPWORD: \",X1[0])\n",
    "print(\"SP. CHAR: \",X2[0])\n",
    "print(\"STEM:     \",X3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode outputs into N- dim one hot encoded matrix\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(Y)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "#print(onehot_encoded)\n",
    "# invert first example to return original, must for return api --how to inverse transform post deploy\n",
    "#inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of one hot encoded output vector:  (39121, 37)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of one hot encoded output vector: \",onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "#max word length of input dataset 'short description colun' \n",
    "df_col_len = int(X3.str.split().str.len().max())\n",
    "print(df_col_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "aa3386af09469682c66cc53a1830a4e42f0e70b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  (33252,)\n",
      "Test samples:  (5869,)\n"
     ]
    }
   ],
   "source": [
    "#Split into train and test(15%)  --x1-after stop words\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X1,onehot_encoded,test_size=0.15)\n",
    "print(\"Training samples: \",X_train.shape)\n",
    "print(\"Test samples: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "aa3386af09469682c66cc53a1830a4e42f0e70b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  (33252,)\n",
      "Test samples:  (5869,)\n"
     ]
    }
   ],
   "source": [
    "#Split into train and test(15%)  --X2\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X2,onehot_encoded,test_size=0.15)\n",
    "print(\"Training samples: \",X_train.shape)\n",
    "print(\"Test samples: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_uuid": "bdca14f2b8cd7bd7cb5ee66fd40ea522217c03c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33252, 20)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and pad , init max sizes...should find way to eff value max words \n",
    "max_words = 10000\n",
    "max_len = 20 #df_col_len #35\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "print(sequences_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "78fff25b8be1de575bff071a2027f3dd2b11b911",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define LSTM  model\n",
    "#tweak parameters/layers to inc. eff, smtimes 1D conv also used\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs) #50 dim\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(num_classes,name='out_layer')(layer)  #num_classes=# of outputs\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "78fff25b8be1de575bff071a2027f3dd2b11b911",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MODEL 2 \n",
    "#tweak parameters/layers to inc. eff, smtimes 1D conv also used\n",
    "def RNN2():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs) #50 dim\n",
    "    layer = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(layer)\n",
    "    #layer = Dense(256,name='FC1')(layer)\n",
    "    #layer = Activation('relu')(layer)\n",
    "    #layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(num_classes,name='out_layer')(layer)  #num_classes=# of outputs\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN2()\n",
    "#model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "78fff25b8be1de575bff071a2027f3dd2b11b911",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MODEL 3 \n",
    "#tweak parameters/layers to inc. eff, smtimes 1D conv also used\n",
    "from keras.layers import Bidirectional\n",
    "def RNN2():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs) #50 dim    \n",
    "    layer = Bidirectional(LSTM(64))(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    #layer = Dense(256,name='FC1')(layer)\n",
    "    #layer = Activation('relu')(layer)\n",
    "    #layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(num_classes,name='out_layer')(layer)  #num_classes=# of outputs\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN2()\n",
    "#model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 20, 50)            500000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               58880     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 37)                4773      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 563,653\n",
      "Trainable params: 563,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "a0ede32d4127e8b4990fd74fe97fadef9e565d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 20, 50)            500000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 37)                9509      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 555,589\n",
      "Trainable params: 555,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\124578\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "#compile model with optimizer -  can use RMSProp or Adam\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "98f6d6318352420ea49c532cda158f715f940f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29926 samples, validate on 3326 samples\n",
      "Epoch 1/15\n",
      "29926/29926 [==============================] - 48s 2ms/step - loss: 1.5627 - acc: 0.6413 - val_loss: 1.3998 - val_acc: 0.6542\n",
      "Epoch 2/15\n",
      "29926/29926 [==============================] - 47s 2ms/step - loss: 1.3240 - acc: 0.6840 - val_loss: 1.2221 - val_acc: 0.7057\n",
      "Epoch 3/15\n",
      "29926/29926 [==============================] - 48s 2ms/step - loss: 1.1908 - acc: 0.7069 - val_loss: 1.1045 - val_acc: 0.7276\n",
      "Epoch 4/15\n",
      "29926/29926 [==============================] - 49s 2ms/step - loss: 1.0894 - acc: 0.7284 - val_loss: 1.0442 - val_acc: 0.7411\n",
      "Epoch 5/15\n",
      "29926/29926 [==============================] - 52s 2ms/step - loss: 1.0210 - acc: 0.7452 - val_loss: 1.0357 - val_acc: 0.7441\n",
      "Epoch 6/15\n",
      "29926/29926 [==============================] - 59s 2ms/step - loss: 0.9682 - acc: 0.7591 - val_loss: 1.0001 - val_acc: 0.7553\n",
      "Epoch 7/15\n",
      "29926/29926 [==============================] - 58s 2ms/step - loss: 0.9215 - acc: 0.7700 - val_loss: 0.9886 - val_acc: 0.7622\n",
      "Epoch 8/15\n",
      "29926/29926 [==============================] - 59s 2ms/step - loss: 0.8811 - acc: 0.7788 - val_loss: 0.9774 - val_acc: 0.7619\n",
      "Epoch 9/15\n",
      "29926/29926 [==============================] - 53s 2ms/step - loss: 0.8444 - acc: 0.7883 - val_loss: 0.9633 - val_acc: 0.7679\n",
      "Epoch 10/15\n",
      "29926/29926 [==============================] - 60s 2ms/step - loss: 0.8101 - acc: 0.7957 - val_loss: 0.9647 - val_acc: 0.7697\n"
     ]
    }
   ],
   "source": [
    "# RUN on MODEL 3 -Bidirectional LSTM\n",
    "#run after X2 stop words and sp char, vocab size 10k, out=37\n",
    "#execute the model, early stopping if model stops converging, batch size can be tweaked. 20% data for validation each epoch\n",
    "history = model.fit(sequences_matrix,Y_train,batch_size=64,epochs=15,\n",
    "          validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "98f6d6318352420ea49c532cda158f715f940f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29926 samples, validate on 3326 samples\n",
      "Epoch 1/15\n",
      "29926/29926 [==============================] - 39s 1ms/step - loss: 1.5947 - acc: 0.6406 - val_loss: 1.4257 - val_acc: 0.6648\n",
      "Epoch 2/15\n",
      "29926/29926 [==============================] - 34s 1ms/step - loss: 1.3217 - acc: 0.6854 - val_loss: 1.2653 - val_acc: 0.6921\n",
      "Epoch 3/15\n",
      "29926/29926 [==============================] - 34s 1ms/step - loss: 1.2016 - acc: 0.7035 - val_loss: 1.2141 - val_acc: 0.6981\n",
      "Epoch 4/15\n",
      "29926/29926 [==============================] - 37s 1ms/step - loss: 1.1324 - acc: 0.7170 - val_loss: 1.1426 - val_acc: 0.7120\n",
      "Epoch 5/15\n",
      "29926/29926 [==============================] - 34s 1ms/step - loss: 1.0630 - acc: 0.7336 - val_loss: 1.1014 - val_acc: 0.7222\n",
      "Epoch 6/15\n",
      "29926/29926 [==============================] - 35s 1ms/step - loss: 1.0069 - acc: 0.7471 - val_loss: 1.0717 - val_acc: 0.7273\n",
      "Epoch 7/15\n",
      "29926/29926 [==============================] - 35s 1ms/step - loss: 0.9620 - acc: 0.7576 - val_loss: 1.0550 - val_acc: 0.7342\n",
      "Epoch 8/15\n",
      "29926/29926 [==============================] - 36s 1ms/step - loss: 0.9243 - acc: 0.7662 - val_loss: 1.0330 - val_acc: 0.7405\n",
      "Epoch 9/15\n",
      "29926/29926 [==============================] - 38s 1ms/step - loss: 0.8912 - acc: 0.7746 - val_loss: 1.0284 - val_acc: 0.7450\n",
      "Epoch 10/15\n",
      "29926/29926 [==============================] - 37s 1ms/step - loss: 0.8621 - acc: 0.7819 - val_loss: 1.0177 - val_acc: 0.7462\n",
      "Epoch 11/15\n",
      "29926/29926 [==============================] - 41s 1ms/step - loss: 0.8341 - acc: 0.7872 - val_loss: 1.0236 - val_acc: 0.7468\n"
     ]
    }
   ],
   "source": [
    "# RUN on MODEL 2 - LSTM 100\n",
    "#run after X1 stop words, vocab size 10k, out=37\n",
    "#execute the model, early stopping if model stops converging, batch size can be tweaked. 20% data for validation each epoch\n",
    "history = model.fit(sequences_matrix,Y_train,batch_size=64,epochs=15,\n",
    "          validation_split=0.15,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "98f6d6318352420ea49c532cda158f715f940f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29926 samples, validate on 3326 samples\n",
      "Epoch 1/15\n",
      "29926/29926 [==============================] - 27s 908us/step - loss: 1.4095 - acc: 0.6703 - val_loss: 1.2082 - val_acc: 0.6978\n",
      "Epoch 2/15\n",
      "29926/29926 [==============================] - 27s 899us/step - loss: 1.1590 - acc: 0.7126 - val_loss: 1.1380 - val_acc: 0.7150\n",
      "Epoch 3/15\n",
      "29926/29926 [==============================] - 29s 960us/step - loss: 1.0701 - acc: 0.7308 - val_loss: 1.1113 - val_acc: 0.7222\n",
      "Epoch 4/15\n",
      "29926/29926 [==============================] - 28s 929us/step - loss: 1.0146 - acc: 0.7413 - val_loss: 1.0774 - val_acc: 0.7297\n",
      "Epoch 5/15\n",
      "29926/29926 [==============================] - 28s 927us/step - loss: 0.9722 - acc: 0.7513 - val_loss: 1.0736 - val_acc: 0.7357\n",
      "Epoch 6/15\n",
      "29926/29926 [==============================] - 28s 944us/step - loss: 0.9406 - acc: 0.7579 - val_loss: 1.1026 - val_acc: 0.7330\n"
     ]
    }
   ],
   "source": [
    "#run after X1 stop words, vocab size 10k, out=37\n",
    "#execute the model, early stopping if model stops converging, batch size can be tweaked. 20% data for validation each epoch\n",
    "history = model.fit(sequences_matrix,Y_train,batch_size=64,epochs=15,\n",
    "          validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "98f6d6318352420ea49c532cda158f715f940f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26601 samples, validate on 6651 samples\n",
      "Epoch 1/15\n",
      "26601/26601 [==============================] - 28s 1ms/step - loss: 1.4284 - acc: 0.6697 - val_loss: 1.2090 - val_acc: 0.7079\n",
      "Epoch 2/15\n",
      "26601/26601 [==============================] - 25s 923us/step - loss: 1.1607 - acc: 0.7112 - val_loss: 1.1181 - val_acc: 0.7271\n",
      "Epoch 3/15\n",
      "26601/26601 [==============================] - 25s 947us/step - loss: 1.0735 - acc: 0.7284 - val_loss: 1.1218 - val_acc: 0.7306\n"
     ]
    }
   ],
   "source": [
    "#run after X3 stemming, vocab size 10k, out=37\n",
    "#execute the model, early stopping if model stops converging, batch size can be tweaked. 20% data for validation each epoch\n",
    "history = model.fit(sequences_matrix,Y_train,batch_size=64,epochs=15,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "98f6d6318352420ea49c532cda158f715f940f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26601 samples, validate on 6651 samples\n",
      "Epoch 1/15\n",
      "26601/26601 [==============================] - 25s 956us/step - loss: 1.4875 - acc: 0.6602 - val_loss: 1.2431 - val_acc: 0.6969\n",
      "Epoch 2/15\n",
      "26601/26601 [==============================] - 28s 1ms/step - loss: 1.1678 - acc: 0.7109 - val_loss: 1.1451 - val_acc: 0.7188\n",
      "Epoch 3/15\n",
      "26601/26601 [==============================] - 28s 1ms/step - loss: 1.0625 - acc: 0.7341 - val_loss: 1.1113 - val_acc: 0.7250\n",
      "Epoch 4/15\n",
      "26601/26601 [==============================] - 26s 964us/step - loss: 0.9936 - acc: 0.7489 - val_loss: 1.1360 - val_acc: 0.7309\n"
     ]
    }
   ],
   "source": [
    "#run after X2 - stop words and remove special chars, vocab size 10k, out=37\n",
    "#execute the model, early stopping if model stops converging, batch size can be tweaked. 20% data for validation each epoch\n",
    "history = model.fit(sequences_matrix,Y_train,batch_size=64,epochs=15,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "98f6d6318352420ea49c532cda158f715f940f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26601 samples, validate on 6651 samples\n",
      "Epoch 1/15\n",
      "26601/26601 [==============================] - 27s 1ms/step - loss: 1.5784 - acc: 0.6397 - val_loss: 1.4379 - val_acc: 0.6429\n",
      "Epoch 2/15\n",
      "26601/26601 [==============================] - 25s 931us/step - loss: 1.3482 - acc: 0.6747 - val_loss: 1.2541 - val_acc: 0.6981\n",
      "Epoch 3/15\n",
      "26601/26601 [==============================] - 26s 987us/step - loss: 1.2276 - acc: 0.6934 - val_loss: 1.1981 - val_acc: 0.7041\n",
      "Epoch 4/15\n",
      "26601/26601 [==============================] - 26s 962us/step - loss: 1.1591 - acc: 0.7029 - val_loss: 1.1441 - val_acc: 0.7139\n",
      "Epoch 5/15\n",
      "26601/26601 [==============================] - 25s 933us/step - loss: 1.1066 - acc: 0.7142 - val_loss: 1.1350 - val_acc: 0.7199\n",
      "Epoch 6/15\n",
      "26601/26601 [==============================] - 25s 923us/step - loss: 1.0614 - acc: 0.7261 - val_loss: 1.1216 - val_acc: 0.7264\n",
      "Epoch 7/15\n",
      "26601/26601 [==============================] - 32s 1ms/step - loss: 1.0197 - acc: 0.7379 - val_loss: 1.1191 - val_acc: 0.7307\n",
      "Epoch 8/15\n",
      "26601/26601 [==============================] - 28s 1ms/step - loss: 0.9677 - acc: 0.7531 - val_loss: 1.1114 - val_acc: 0.7378\n",
      "Epoch 9/15\n",
      "26601/26601 [==============================] - 26s 960us/step - loss: 0.9241 - acc: 0.7635 - val_loss: 1.1089 - val_acc: 0.7385\n",
      "Epoch 10/15\n",
      "26601/26601 [==============================] - 27s 1ms/step - loss: 0.8853 - acc: 0.7758 - val_loss: 1.1183 - val_acc: 0.7396\n"
     ]
    }
   ],
   "source": [
    "#run on Orig X\n",
    "#execute the model, early stopping if model stops converging, batch size can be tweaked. 20% data for validation each epoch\n",
    "history = model.fit(sequences_matrix,Y_train,batch_size=64,epochs=15,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model to file\n",
    "model.save('model_snow_v1_1_lstm100_37out_0911.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##notes\n",
    "#>200: input=24k, output classes=27,BS=64,RMS,LSTM(64,256,),dropout 0.5,epochs=9, eff=75%; if BS=128,epoch=2,eff same\n",
    "#>200: input=55kk,output classes=57,BS=64,RMS,LSTM(64,256,),drop 0.5,epoch 5, eff=64%\n",
    "#>500: input=43k,out classes=20,BS=64,RMS,LSTM(64,256,),drop 0.5,epoch 9, eff=80.8%\n",
    "#drop duplicates,>200: input=39k,out=37,BS=64,RMS,LSTM(64,256,),drop 0.5,epoch 8, eff=73%, test=.1, val=.15\n",
    "#drop duplicates,>300: input=35k,out=20,BS=64,RMS,LSTM(64,256,),drop 0.5,epoch 6, eff=80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "80036135a11387d952becaf2fecf653a65c02328",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create sequences for test data\n",
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "3e121ab83f4a0b9f7376ab24aa25d67051171f89",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5869/5869 [==============================] - 3s 588us/step\n",
      "Test set\n",
      "  Loss: 1.080\n",
      "  Accuracy: 74.629%\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy on test data\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3%}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 2)\n"
     ]
    }
   ],
   "source": [
    "#load unseen data for validation\n",
    "df_val = read_excel('validate2018.xlsx', sheet_name = 'Sheet1')\n",
    "Xnew=df_val.Description\n",
    "Ynew_orig = df_val.Group\n",
    "print(Xnew.shape,Ynew_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert unseen data to word tokens\n",
    "Xnew_2 = sequence.pad_sequences(tok.texts_to_sequences(Xnew),maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict output on unseen data\n",
    "ynew = model.predict(Xnew_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=zOther Clinical Application Issue || Helpdesk - Clinical Application Issue,\n",
      "Predicted=['DH-Helpdesk HDAG'],\n",
      "Original=DH-Helpdesk HDAG\n",
      "\n",
      "X=Monitor ||  How to increase the brightness || ,\n",
      "Predicted=['DH-Enterprise IT Service Cntr'],\n",
      "Original=DH-Enterprise IT Service Cntr\n",
      "\n",
      "X=Outlook || Outlook Application Issue,\n",
      "Predicted=['DH-Helpdesk RRE'],\n",
      "Original=DH-Helpdesk RRE\n",
      "\n",
      "X=Phone || Phone Issue,\n",
      "Predicted=['DH-NC-Telcom Sacramento'],\n",
      "Original=DH-NC-Telcom Sacramento\n",
      "\n",
      "X=Kyocera Printer Issue || Kyocera Printer Issue,\n",
      "Predicted=['DH-MPS Kyocera'],\n",
      "Original=DH-MPS Kyocera\n",
      "\n",
      "X=Emergency Account Disablement || user ID : aguise001 || Need to be disabled Immediately ,\n",
      "Predicted=['DHE-SecAdmin'],\n",
      "Original=DHE-SecAdmin\n",
      "\n",
      "X=Network, account lock, username : jfunk002,\n",
      "Predicted=['DH-Enterprise IT Service Cntr'],\n",
      "Original=DH-Enterprise IT Service Cntr\n",
      "\n",
      "X=printer || patient data not printing up from cerner ,\n",
      "Predicted=['DH-ClinApps NAS'],\n",
      "Original=DH-ClinApps HIM\n",
      "\n",
      "X=network account  || password reset || user name : nmatson || HDUV  done ||  reset password || issue resolved ,\n",
      "Predicted=['DH-Enterprise IT Service Cntr'],\n",
      "Original=DH-Enterprise IT Service Cntr\n",
      "\n",
      "X=Unable to print patient cardiac strips from central monitor at nurses station in TICU Trauma ICU.Other Desktop Issue || Other Desktop Issue,\n",
      "Predicted=['DH-SW-EUS StJoseph'],\n",
      "Original=DH-SW-EUS Chandler\n",
      "\n",
      "X=Cerner // Access Level is incorrectly provisioned  // FW: ***Cerner*** ,\n",
      "Predicted=['DH-ClinApps NAS'],\n",
      "Original=DHE-SecAdmin\n",
      "\n",
      "X=Label Printer Issue || Label Printer Issue,\n",
      "Predicted=['DH-Helpdesk HDAG'],\n",
      "Original=DH-NC-EUS Redding\n",
      "\n",
      "X=Trackingshell || Patient showing twice || Dept: ER || Loc: California Hospital M,\n",
      "Predicted=['DH-ClinApps NAS'],\n",
      "Original=DH-ClinApps NAS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print unseen data, predicted value, original value\n",
    "for i in range(len(Xnew)):\n",
    "\tprint(\"X=%s,\\nPredicted=%s,\\nOriginal=%s\\n\" % (Xnew[i], label_encoder.inverse_transform([argmax(ynew[i, :])]),Ynew_orig[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
